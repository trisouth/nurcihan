{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "211d643e-045b-429e-aed9-75d1eabc25bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum, avg, col, lit\n",
    "import os\n",
    "from custom_exception import CustomException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "de744bdb-139f-4501-866f-3e1ca27ffd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define user variables\n",
    "yellow_taxi_path = os.path.join(os.getcwd(), \"/rawdata/yellow_tripdata_2021-01.csv\")\n",
    "green_taxi_path = os.path.join(os.getcwd(), \"/rawdata/green_tripdata_2021-01.csv\")\n",
    "bronze_dir = os.path.join(os.getcwd(), \"/pipelinedata/Bronze\")\n",
    "silver_dir = os.path.join(os.getcwd(), \"/pipelinedata/Silver\")\n",
    "gold_dir = os.path.join(os.getcwd(), \"/pipelinedata/Gold\")\n",
    "\n",
    "yellow_tripdata_parquet = f\"{bronze_dir}/yellow_tripdata.parquet\"\n",
    "green_tripdata_parquet = f\"{bronze_dir}/green_tripdata.parquet\"\n",
    "merged_tripdata_parquet = f\"{silver_dir}/merged_tripdata.parquet\"\n",
    "\n",
    "yellow_tripdata_valid_parquet = f\"{silver_dir}/yellow_tripdata_valid.parquet\"\n",
    "yellow_tripdata_invalid_parquet = f\"{silver_dir}/yellow_tripdata_invalid.csv\"\n",
    "\n",
    "locations_csv = f\"{gold_dir}/locations.csv\"\n",
    "vendors_csv = f\"{gold_dir}/vendors.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5a7e2746-a695-4bb8-b3bd-11674b0e059e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define application variables\n",
    "spark = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e629ac04-2218-472a-bf84-f3ba5ca6ee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "\n",
    "    # Create SparkSession\n",
    "    try:\n",
    "        return SparkSession.builder.appName(\"TaxiDataPipeline\").master(\"local\").getOrCreate()\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating Spark session: {e}\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "77768b6b-4e63-4229-b2a1-3419f397eabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def housekeeping():\n",
    "    try:\n",
    "        # if the directory contains files, remove them\n",
    "        if os.path.exists(bronze_dir) and os.path.isdir(bronze_dir):\n",
    "            os.system(f\"rm -rf {bronze_dir}/*\")\n",
    "        \n",
    "        if os.path.exists(silver_dir) and os.path.isdir(silver_dir):\n",
    "            os.system(f\"rm -rf {silver_dir}/*\")\n",
    "\n",
    "        if os.path.exists(gold_dir) and os.path.isdir(gold_dir):\n",
    "            os.system(f\"rm -rf {gold_dir}/*\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error cleaning target directories: {e}\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "47dfc9b9-db7e-4841-a6c4-32f6eb701310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_files(spark, file_path):\n",
    "\n",
    "    try:\n",
    "\n",
    "        # read the csv file and return the dataframe \n",
    "        return spark.read.csv(file_path, header=True)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading data in step1: {e}\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b6d7068d-3bd5-4d58-8339-01f0d7458c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df_as_parquet(df, target_destination):\n",
    "\n",
    "    try:\n",
    "\n",
    "        # write the data in dataframes into target parquet files\n",
    "        df.write.parquet(target_destination)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing data in parquet format to : {e}\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9401a7cb-caa2-4fc4-811b-f9df8dbb7385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df_as_csv(df, target_destination):\n",
    "\n",
    "    try:\n",
    "\n",
    "        # write the data in dataframes into target csv files\n",
    "        df.write.csv(target_destination)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing data in csv format to : {e}\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bfd24589-cbfb-4c4b-8205-3fc077f34055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_transform_columns(df, taxi_type):\n",
    "\n",
    "    try:\n",
    "\n",
    "        # rename columns for standardization\n",
    "        df = df.withColumnRenamed(\"VendorID\", \"VendorId\") \\\n",
    "            .withColumnRenamed(\"PULocationID\", \"PickUpLocationId\") \\\n",
    "            .withColumnRenamed(\"DOLocationID\", \"DropOffLocationId\") \\\n",
    "            .withColumnRenamed(\"passenger_count\", \"PassengerCount\") \\\n",
    "            .withColumnRenamed(\"trip_distance\", \"TripDistance\") \\\n",
    "            .withColumnRenamed(\"tip_amount\", \"TipAmount\") \\\n",
    "            .withColumnRenamed(\"total_amount\", \"TotalAmount\")\n",
    "        \n",
    "        if taxi_type == 'Y':\n",
    "\n",
    "            df = df.withColumnRenamed(\"tpep_pickup_datetime\", \"PickUpDateTime\") \\\n",
    "                .withColumnRenamed(\"tpep_dropoff_datetime\", \"DropOffDateTime\")\n",
    "            \n",
    "        elif taxi_type == 'G':\n",
    "\n",
    "            df = df.withColumnRenamed(\"lpep_pickup_datetime\", \"PickUpDateTime\") \\\n",
    "                .withColumnRenamed(\"lpep_dropoff_datetime\", \"DropOffDateTime\")\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # raise an exception of unknown taxi type\n",
    "            raise CustomException(f\"Unknown taxi type : {taxi_type}\", 1001)\n",
    "        \n",
    "        return df.select(\"VendorId\", \"PickUpDateTime\", \"DropOffDateTime\", \"PickUpLocationId\", \"DropOffLocationId\", \"PassengerCount\", \"TripDistance\", \"TipAmount\", \"TotalAmount\")\n",
    "    \n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"Error renaming columns : {e} for taxi_type : {taxi_type}\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2ea3b83a-4cf9-4d41-a5a0-508e17a351db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dataframes(first_df, second_df):\n",
    "\n",
    "    try:\n",
    "\n",
    "        # merge dataframes\n",
    "        return first_df.unionByName(second_df)\n",
    "    \n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"Error merging dataframes : {e}\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "df001058-5c71-458b-a0e7-225c7fde2a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df(df, col):\n",
    "\n",
    "    try:\n",
    "\n",
    "        # filter the dataframes\n",
    "        return df.filter(col)\n",
    "    \n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"Error filtering dataframes : {e}\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b891cac0-9f2f-468a-a7bc-d7bff5e894e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_null_values(df, col, value):\n",
    "\n",
    "    try:\n",
    "\n",
    "        # replace null values with the given value\n",
    "        return df.fillna(value, subset=[col])\n",
    "    \n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"Error replacing null values : {e}\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "29b1f53a-3d21-4fea-aab0-30c75309dbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplicate_df(df, cols):\n",
    "\n",
    "    try:\n",
    "\n",
    "        # deduplicate the dataframes\n",
    "        return df.dropDuplicates(cols)\n",
    "    \n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"Error deduplicating dataframes : {e}\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "73cff274-7871-4716-a32a-ce2a605dcf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_locations_df(df):\n",
    "\n",
    "    try:\n",
    "        \n",
    "        # Calculate aggregations for Locations\n",
    "        return df.groupBy(\"PickUpLocationId\").agg(\n",
    "            sum(\"TotalAmount\").alias(\"TotalFares\"),\n",
    "            sum(\"TipAmount\").alias(\"TotalTips\"),\n",
    "            avg(\"TripDistance\").alias(\"AverageDistance\")\n",
    "        ).withColumn(\"LocationType\", lit(\"PickUp\")) \\\n",
    "        .withColumnRenamed(\"PickUpLocationId\", \"LocationId\")\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"Error aggregating locations : {e}\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "af44772e-f7ab-4711-a2f0-3d5ef7c0b22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_dropoffs(df):\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Calculate the average distance by dropoff location separately\n",
    "        return df.groupBy(\"DropOffLocationId\").agg(\n",
    "            avg(\"TripDistance\").alias(\"AverageDistance\")\n",
    "            ).withColumn(\"TotalFares\", lit(0)) \\\n",
    "            .withColumn(\"TotalTips\", lit(0)) \\\n",
    "            .withColumn(\"LocationType\", lit(\"DropOff\")) \\\n",
    "            .withColumnRenamed(\"DropOffLocationId\", \"LocationId\") \\\n",
    "            .select(\"LocationId\", \"TotalFares\", \"TotalTips\", \"AverageDistance\", \"LocationType\")\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"Error aggregating dropoffs : {e}\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5bc62c9a-3a9e-48fe-a4e4-524c50ba223a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_vendors(df):\n",
    "    \n",
    "    try:\n",
    "        # Calculate aggregations for Vendors\n",
    "        return df.groupBy(\"VendorId\").agg(\n",
    "            sum(\"TotalAmount\").alias(\"TotalFares\"),\n",
    "            sum(\"TipAmount\").alias(\"TotalTips\"),\n",
    "            avg(\"TotalAmount\").alias(\"AverageFare\"),\n",
    "            avg(\"TipAmount\").alias(\"AverageTips\")\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"Error aggregating vendors : {e}\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fc6cd9d6-4e7b-4f98-8229-6f96685d61ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_spark_session():\n",
    "    if spark:\n",
    "        spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "43dab688-91f8-478b-8672-0b44f081bb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # create sparkSession\n",
    "    spark = create_spark_session()\n",
    "\n",
    "    # perform housekeeping, delete files from previous runs\n",
    "    housekeeping()\n",
    "\n",
    "    # step 1 - load the raw data into initial df's\n",
    "    yellow_df = read_csv_files(spark=spark, file_path=yellow_taxi_path)\n",
    "    green_df = read_csv_files(spark=spark, file_path=green_taxi_path)\n",
    "\n",
    "    # step 1 - save the raw dataframes as parquet files\n",
    "    save_df_as_parquet(df=yellow_df, target_destination=yellow_tripdata_parquet)\n",
    "    save_df_as_parquet(df=green_df, target_destination=green_tripdata_parquet)\n",
    "    \n",
    "    # step 2 - rename and reduce columns\n",
    "    yellow_df = rename_transform_columns(df=yellow_df, taxi_type='Y')\n",
    "    green_df = rename_transform_columns(df=green_df, taxi_type='G')\n",
    "\n",
    "    merged_df = merge_dataframes(first_df=yellow_df, second_df=green_df)\n",
    "    save_df_as_parquet(df=merged_df, target_destination=merged_tripdata_parquet)\n",
    "\n",
    "    # step 3 - apply validation rules\n",
    "\n",
    "    # validation rule 1 based on the passenger count\n",
    "    valid_df = filter_df(df=merged_df, col=col(\"PassengerCount\") >= 1)\n",
    "    invalid_df = filter_df(merged_df, (col(\"PassengerCount\") < 1) | col(\"PassengerCount\").isNull())\n",
    "\n",
    "    # validation rule 2 based on the vendor id\n",
    "    valid_df = replace_null_values(df=valid_df, col=\"VendorId\", value=999)\n",
    "\n",
    "    save_df_as_parquet(df=valid_df, target_destination=yellow_tripdata_valid_parquet)\n",
    "    save_df_as_csv(df=invalid_df, target_destination=yellow_tripdata_invalid_parquet)\n",
    "\n",
    "    # step 4 - deduplicate the data\n",
    "    deduped_df = deduplicate_df(df=valid_df, cols=[\"PickUpLocationId\", \"PickUpDateTime\", \"DropOffDateTime\", \"DropOffLocationId\", \"VendorId\"])\n",
    "\n",
    "    # step 5 - apply aggregations\n",
    "    locations_df = aggregate_locations_df(df=deduped_df)\n",
    "    dropoff_df = aggregate_dropoffs(df=deduped_df)\n",
    "\n",
    "    merged_df = merge_dataframes(locations_df, dropoff_df)\n",
    "\n",
    "    vendors_df = aggregate_vendors(df=deduped_df)\n",
    "\n",
    "    save_df_as_csv(df=merged_df, target_destination=locations_csv)\n",
    "    save_df_as_csv(df=vendors_df, target_destination=vendors_csv)\n",
    "    \n",
    "    stop_spark_session()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
