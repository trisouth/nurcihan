{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a4dc1161-eaff-4fd3-8c67-544b1fb25f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum, avg, col, lit\n",
    "import os\n",
    "from custom_exception import CustomException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9b9686d0-0b0f-44ae-9b24-aac54f5afd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define user variables\n",
    "yellow_taxi_path = os.path.join(os.getcwd(), \"/rawdata/yellow_tripdata_2021-01.csv\")\n",
    "green_taxi_path = os.path.join(os.getcwd(), \"/rawdata/green_tripdata_2021-01.csv\")\n",
    "bronze_dir = os.path.join(os.getcwd(), \"/pipelinedata/Bronze\")\n",
    "silver_dir = os.path.join(os.getcwd(), \"/pipelinedata/Silver\")\n",
    "gold_dir = os.path.join(os.getcwd(), \"/pipelinedata/Gold\")\n",
    "\n",
    "yellow_tripdata_parquet = f\"{bronze_dir}/yellow_tripdata.parquet\"\n",
    "green_tripdata_parquet = f\"{bronze_dir}/green_tripdata.parquet\"\n",
    "merged_tripdata_parquet = f\"{silver_dir}/merged_tripdata.parquet\"\n",
    "\n",
    "yellow_tripdata_valid_parquet = f\"{silver_dir}/yellow_tripdata_valid.parquet\"\n",
    "yellow_tripdata_invalid_parquet = f\"{silver_dir}/yellow_tripdata_invalid.csv\"\n",
    "\n",
    "locations_csv = f\"{gold_dir}/locations.csv\"\n",
    "vendors_csv = f\"{gold_dir}/vendors.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5aca998c-3645-4e43-b225-adb8e7ba4b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define application variables\n",
    "spark = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dfa594bb-2aa8-4243-a48c-90c6eda5eaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "\n",
    "    # Create SparkSession\n",
    "    try:\n",
    "        return SparkSession.builder.appName(\"TaxiDataPipeline\").master(\"local\").getOrCreate()\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating Spark session: {e}\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0955b0fc-0abc-456e-8518-575b4f6018e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_files(spark, file_path):\n",
    "\n",
    "    try:\n",
    "\n",
    "        # read the csv file and return the dataframe \n",
    "        return spark.read.csv(file_path, header=True)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading data in step1: {e}\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "af6e7050-9f74-4587-a6b4-473c1b642f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_transform_columns(df, taxi_type):\n",
    "\n",
    "    try:\n",
    "\n",
    "        # rename columns for standardization\n",
    "        df = df.withColumnRenamed(\"VendorID\", \"VendorId\") \\\n",
    "            .withColumnRenamed(\"PULocationID\", \"PickUpLocationId\") \\\n",
    "            .withColumnRenamed(\"DOLocationID\", \"DropOffLocationId\") \\\n",
    "            .withColumnRenamed(\"passenger_count\", \"PassengerCount\") \\\n",
    "            .withColumnRenamed(\"trip_distance\", \"TripDistance\") \\\n",
    "            .withColumnRenamed(\"tip_amount\", \"TipAmount\") \\\n",
    "            .withColumnRenamed(\"total_amount\", \"TotalAmount\")\n",
    "        \n",
    "        if taxi_type == 'Y':\n",
    "\n",
    "            df = df.withColumnRenamed(\"tpep_pickup_datetime\", \"PickUpDateTime\") \\\n",
    "                .withColumnRenamed(\"tpep_dropoff_datetime\", \"DropOffDateTime\")\n",
    "            \n",
    "        elif taxi_type == 'G':\n",
    "\n",
    "            df = df.withColumnRenamed(\"lpep_pickup_datetime\", \"PickUpDateTime\") \\\n",
    "                .withColumnRenamed(\"lpep_dropoff_datetime\", \"DropOffDateTime\")\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # raise an exception of unknown taxi type\n",
    "            raise CustomException(f\"Unknown taxi type : {taxi_type}\", 1001)\n",
    "        \n",
    "        return df.select(\"VendorId\", \"PickUpDateTime\", \"DropOffDateTime\", \"PickUpLocationId\", \"DropOffLocationId\", \"PassengerCount\", \"TripDistance\", \"TipAmount\", \"TotalAmount\")\n",
    "    \n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"Error renaming columns : {e} for taxi_type : {taxi_type}\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8cf8047d-bd10-4734-87da-0671eeddb14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dataframes(first_df, second_df):\n",
    "\n",
    "    try:\n",
    "\n",
    "        # merge dataframes\n",
    "        return first_df.unionByName(second_df)\n",
    "    \n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"Error merging dataframes : {e}\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3cad063d-d8b9-4b45-a4c8-0b5bb68a69ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df(df, col):\n",
    "\n",
    "    try:\n",
    "\n",
    "        # filter the dataframes\n",
    "        return df.filter(col)\n",
    "    \n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"Error filtering dataframes : {e}\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bc9ef73e-2b46-4b8c-9be8-6034117f4909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_null_values(df, col, value):\n",
    "\n",
    "    try:\n",
    "\n",
    "        # replace null values with the given value\n",
    "        return df.fillna(value, subset=[col])\n",
    "    \n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"Error replacing null values : {e}\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2a6378d3-0a5f-4ede-a404-d593c591eb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplicate_df(df, cols):\n",
    "\n",
    "    try:\n",
    "\n",
    "        # deduplicate the dataframes\n",
    "        return df.dropDuplicates(cols)\n",
    "    \n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"Error deduplicating dataframes : {e}\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4865a7cc-8683-46a5-8494-47c34b93d842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_locations_df(df):\n",
    "\n",
    "    try:\n",
    "        \n",
    "        # Calculate aggregations for Locations\n",
    "        return df.groupBy(\"PickUpLocationId\").agg(\n",
    "            sum(\"TotalAmount\").alias(\"TotalFares\"),\n",
    "            sum(\"TipAmount\").alias(\"TotalTips\"),\n",
    "            avg(\"TripDistance\").alias(\"AverageDistance\")\n",
    "        ).withColumn(\"LocationType\", lit(\"PickUp\")) \\\n",
    "        .withColumnRenamed(\"PickUpLocationId\", \"LocationId\")\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"Error aggregating locations : {e}\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "369b54ee-5274-44ae-b6d6-1d02015ab6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_dropoffs(df):\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Calculate the average distance by dropoff location separately\n",
    "        return df.groupBy(\"DropOffLocationId\").agg(\n",
    "            avg(\"TripDistance\").alias(\"AverageDistance\")\n",
    "            ).withColumn(\"TotalFares\", lit(0)) \\\n",
    "            .withColumn(\"TotalTips\", lit(0)) \\\n",
    "            .withColumn(\"LocationType\", lit(\"DropOff\")) \\\n",
    "            .withColumnRenamed(\"DropOffLocationId\", \"LocationId\") \\\n",
    "            .select(\"LocationId\", \"TotalFares\", \"TotalTips\", \"AverageDistance\", \"LocationType\")\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"Error aggregating dropoffs : {e}\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7687021c-67fc-4bc7-a9ea-9e1968ef6ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_vendors(df):\n",
    "    \n",
    "    try:\n",
    "        # Calculate aggregations for Vendors\n",
    "        return df.groupBy(\"VendorId\").agg(\n",
    "            sum(\"TotalAmount\").alias(\"TotalFares\"),\n",
    "            sum(\"TipAmount\").alias(\"TotalTips\"),\n",
    "            avg(\"TotalAmount\").alias(\"AverageFare\"),\n",
    "            avg(\"TipAmount\").alias(\"AverageTips\")\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\"Error aggregating vendors : {e}\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3d6efffe-a2b7-44ad-a670-0995d31e914c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_spark_session():\n",
    "    if spark:\n",
    "        spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9828a913-b2ee-4e61-be0e-6d138947560f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows in the yellow is: 1369819\n",
      "The number of rows in the green is: 76539\n",
      "+--------+-------------------+-------------------+----------------+-----------------+--------------+------------+---------+-----------+\n",
      "|VendorId|     PickUpDateTime|    DropOffDateTime|PickUpLocationId|DropOffLocationId|PassengerCount|TripDistance|TipAmount|TotalAmount|\n",
      "+--------+-------------------+-------------------+----------------+-----------------+--------------+------------+---------+-----------+\n",
      "|       2|2021-01-01 00:42:11|2021-01-01 00:44:24|              50|              142|             5|         .81|        0|        8.3|\n",
      "|       2|2021-01-01 00:43:41|2021-01-01 00:48:17|             239|              142|             3|         .93|        0|        9.3|\n",
      "|       1|2021-01-01 00:16:27|2021-01-01 00:25:36|             249|              137|             0|        2.20|        0|       12.8|\n",
      "|       2|2021-01-01 00:55:19|2021-01-01 00:58:45|             263|               75|             2|        1.05|     1.86|      11.16|\n",
      "|       2|2021-01-01 00:34:36|2021-01-01 00:44:07|             229|              229|             1|         .27|        0|       10.8|\n",
      "|       2|2021-01-01 00:23:01|2021-01-01 00:40:05|             261|               48|             1|        4.09|     3.86|      23.16|\n",
      "|       2|2021-01-01 00:30:06|2021-01-01 00:41:00|             125|               48|             1|        3.26|        0|       15.3|\n",
      "|       1|2021-01-01 00:34:38|2021-01-01 00:37:27|             141|              263|             2|        1.10|      2.5|       11.3|\n",
      "|       2|2021-01-01 00:38:55|2021-01-01 00:44:14|             140|              229|             1|        1.24|     2.58|      12.88|\n",
      "|       2|2021-01-01 00:11:49|2021-01-01 00:17:32|             137|              232|             1|        2.44|        0|       12.8|\n",
      "+--------+-------------------+-------------------+----------------+-----------------+--------------+------------+---------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------+-------------------+-------------------+----------------+-----------------+--------------+------------+---------+-----------+\n",
      "|VendorId|     PickUpDateTime|    DropOffDateTime|PickUpLocationId|DropOffLocationId|PassengerCount|TripDistance|TipAmount|TotalAmount|\n",
      "+--------+-------------------+-------------------+----------------+-----------------+--------------+------------+---------+-----------+\n",
      "|       2|2021-01-01 00:16:36|2021-01-01 00:16:40|             265|              265|             3|         .00|        0|       52.8|\n",
      "|       2|2021-01-01 00:31:14|2021-01-01 00:55:07|             244|              244|             2|        3.39|        0|       19.3|\n",
      "|       2|2021-01-01 00:25:54|2021-01-01 00:28:20|              74|               75|             1|         .68|        0|        5.3|\n",
      "|       1|2021-01-01 00:56:41|2021-01-01 01:13:31|             259|              116|             1|         .00|        0|         29|\n",
      "|       2|2021-01-01 00:15:41|2021-01-01 00:18:57|             166|               41|             1|         .65|        0|        5.8|\n",
      "|       2|2021-01-01 00:35:29|2021-01-01 00:55:15|              74|              247|             1|        3.64|        0|       13.3|\n",
      "|       2|2021-01-01 00:28:19|2021-01-01 00:32:16|              74|               74|             1|         .63|        0|        6.3|\n",
      "|       1|2021-01-01 01:45:34|2021-01-01 01:59:16|             173|               56|             1|        1.50|     2.75|      16.55|\n",
      "|       2|2021-01-01 01:06:08|2021-01-01 01:25:52|              41|              233|             1|        5.03|        0|      21.55|\n",
      "|       2|2021-01-01 01:04:25|2021-01-01 01:17:37|              74|              168|             1|        2.86|        0|       13.3|\n",
      "+--------+-------------------+-------------------+----------------+-----------------+--------------+------------+---------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "The number of rows in the valid is: 1285086\n",
      "The number of rows in the invalid is: 161272\n",
      "+--------------+-------+\n",
      "|PassengerCount|  count|\n",
      "+--------------+-------+\n",
      "|             7|      5|\n",
      "|             3|  44262|\n",
      "|             8|      4|\n",
      "|             0|  26873|\n",
      "|          NULL| 134399|\n",
      "|             5|  31800|\n",
      "|             6|  25682|\n",
      "|             1|1002732|\n",
      "|             4|  16543|\n",
      "|             2| 164058|\n",
      "+--------------+-------+\n",
      "\n",
      "The number of rows in the locations is: 256\n",
      "The number of rows in the vendors is: 2\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # create sparkSession\n",
    "    spark = create_spark_session()\n",
    "\n",
    "    # step 1 - load the raw data into initial df's\n",
    "    yellow_df = read_csv_files(spark=spark, file_path=yellow_taxi_path)\n",
    "    green_df = read_csv_files(spark=spark, file_path=green_taxi_path)\n",
    "\n",
    "    print(f\"The number of rows in the yellow is: {yellow_df.count()}\")\n",
    "    print(f\"The number of rows in the green is: {green_df.count()}\")\n",
    "\n",
    "    # step 2 - rename and reduce columns\n",
    "    yellow_df = rename_transform_columns(df=yellow_df, taxi_type='Y')\n",
    "    green_df = rename_transform_columns(df=green_df, taxi_type='G')\n",
    "\n",
    "    sample_yellow = yellow_df.sample(withReplacement=False, fraction=0.1)\n",
    "    sample_green = green_df.sample(withReplacement=False, fraction=0.1)\n",
    "\n",
    "    sample_yellow.show(10)\n",
    "    sample_green.show(10)\n",
    "\n",
    "    merged_df = merge_dataframes(first_df=yellow_df, second_df=green_df)\n",
    "\n",
    "    # Group by the column and count distinct occurrences\n",
    "    distinct_counts = merged_df.groupBy(\"PassengerCount\").count()\n",
    "\n",
    "    # Show the results\n",
    "    distinct_counts.show()\n",
    "    \n",
    "    # step 3 - apply validation rules\n",
    "\n",
    "    # validation rule 1 based on the passenger count\n",
    "    valid_df = filter_df(df=merged_df, col=col(\"PassengerCount\") >= 1)\n",
    "    invalid_df = filter_df(merged_df, (col(\"PassengerCount\") < 1) | col(\"PassengerCount\").isNull())\n",
    "\n",
    "    print(f\"The number of rows in the valid is: {valid_df.count()}\")\n",
    "    print(f\"The number of rows in the invalid is: {invalid_df.count()}\")\n",
    "\n",
    "    # validation rule 2 based on the vendor id\n",
    "    valid_df = replace_null_values(df=valid_df, col=\"VendorId\", value=999)\n",
    "\n",
    "    # Group by the column and count distinct occurrences\n",
    "    distinct_counts = valid_df.groupBy(\"VendorId\").count()\n",
    "\n",
    "    # Show the results\n",
    "    distinct_counts.show()\n",
    "    \n",
    "    # step 4 - deduplicate the data\n",
    "    deduped_df = deduplicate_df(df=valid_df, cols=[\"PickUpLocationId\", \"PickUpDateTime\", \"DropOffDateTime\", \"DropOffLocationId\", \"VendorId\"])\n",
    "\n",
    "    # step 5 - apply aggregations\n",
    "    locations_df = aggregate_locations_df(df=deduped_df)\n",
    "    dropoff_df = aggregate_dropoffs(df=deduped_df)\n",
    "\n",
    "    merged_df = merge_dataframes(locations_df, dropoff_df)\n",
    "\n",
    "    vendors_df = aggregate_vendors(df=deduped_df)\n",
    "\n",
    "    print(f\"The number of rows in the locations is: {locations_df.count()}\")\n",
    "    print(f\"The number of rows in the vendors is: {vendors_df.count()}\")\n",
    "    \n",
    "\n",
    "    stop_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7234ff-990e-40b1-9acf-f31df0d8a1bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
